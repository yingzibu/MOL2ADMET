{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMoD9+aqJHTW1CPjiy8Wvxp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yingzibu/MOL2ADMET/blob/main/examples/experiments/VAE/ADMET_train_VAE_11_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU16yl_mRr4i",
        "outputId": "b3f39402-ff5a-46d1-d368-4c2bfb9c3edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install PyTDC --quiet\n",
        "! pip install selfies  --quiet\n",
        "! pip install pubchempy --quiet\n",
        "! pip install rdkit --quiet\n",
        "! pip install mycolorpy --quiet\n",
        "! pip install dgllife --quiet\n",
        "! pip install molvs --quiet\n",
        "! pip install dgl -f https://data.dgl.ai/wheels/cu118/repo.html --quiet\n",
        "! pip install dglgo -f https://data.dgl.ai/wheels-test/repo.html --quiet\n",
        "\n",
        "! pip install DeepPurpose --quiet\n",
        "! pip install git+https://github.com/bp-kelley/descriptastorus --quiet\n",
        "! pip install pandas-flavor --quiet"
      ],
      "metadata": {
        "id": "2DD17MZKRy32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "CHMLSUhLSdQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/ADMET"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lo1XEYSSBun",
        "outputId": "f4412bea-59f2-4659-8905-08662f545e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ADMET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scripts.func_utils import *\n",
        "from scripts.yaml_utils import *\n",
        "from scripts.eval_utils import *\n",
        "from scripts.preprocess_mols import *\n",
        "from scripts.model_architecture import *\n",
        "from scripts.dataset import *\n",
        "from scripts.train import *\n",
        "from tdc.utils import retrieve_label_name_list\n",
        "import pandas as pd\n",
        "from scripts.get_vocab import *\n",
        "\n",
        "from tdc.single_pred import ADME\n",
        "from tdc.single_pred import Tox\n",
        "label_list = retrieve_label_name_list('herg_central')\n",
        "\n",
        "# clean_files(path = '/content/drive/MyDrive/ADMET/',\n",
        "#             file_types = ['pth', 'bin', 'pt', 'yml'])\n",
        "\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "def warn(*args, **kwargs): pass\n",
        "warnings.warn = warn\n",
        "\n",
        "\n",
        "def plot_dim_reduced(mol_info, label, task_type, dim_reduct='PCA',\n",
        "                     title=None, savepath=None, savename=None):\n",
        "    \"\"\"\n",
        "    param mol_info: could be MACCS Fingerprint\n",
        "    param label: label of data\n",
        "    param task_type: [True, False], True:regression; False: classification\n",
        "    param dim_reduct\" : ['PCA', 't-SNE']\n",
        "    param title: None or string, the name of the plot\n",
        "    Return figure.png saved at dim_reduct/title.png\n",
        "    \"\"\"\n",
        "    features, labels = mol_info.copy(), label.copy()\n",
        "    n_components = 2\n",
        "    if dim_reduct == 'PCA':\n",
        "        pca = PCA(n_components=n_components)\n",
        "        pca.fit(features)\n",
        "        features = StandardScaler().fit_transform(features)\n",
        "        features = pd.DataFrame(data = pca.transform(features))\n",
        "        ax_label = 'principle component'\n",
        "    elif dim_reduct=='t-SNE':\n",
        "        features = TSNE(n_components=n_components).fit_transform(features)\n",
        "        features = MinMaxScaler().fit_transform(features)\n",
        "        features = pd.DataFrame(np.transpose((features[:,0],features[:,1])))\n",
        "        ax_label = 't-SNE'\n",
        "    else: print(\"\"\"Error! dim_reduct should be 'PCA' or 't-SNE'\"\"\"); return\n",
        "\n",
        "    columns = [f'{ax_label} {i+1}' for i in range(n_components)]\n",
        "    # features = pd.DataFrame(data = pca.transform(features), columns=columns)\n",
        "    features.columns = columns\n",
        "    features['label'] = labels\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "    # f, ax = plt.subplots(figsize=(6, 6))\n",
        "    f, ax = plt.subplots()\n",
        "\n",
        "    param_dict = {'x': columns[0],\n",
        "                'y': columns[1],\n",
        "                'hue':'label',\n",
        "                'palette': 'RdBu',\n",
        "                'data': features,\n",
        "                's': 10,\n",
        "                'ax':ax}\n",
        "\n",
        "    # sns.despine(f, left=True, bottom=False)\n",
        "    sns.scatterplot(**param_dict)\n",
        "\n",
        "    if task_type == True: # regression task, color bar for labels\n",
        "        norm = plt.Normalize(labels.min(), labels.max())\n",
        "        scalarmap = plt.cm.ScalarMappable(cmap=param_dict['palette'], norm=norm)\n",
        "        scalarmap.set_array([])\n",
        "        ax.figure.colorbar(scalarmap)\n",
        "        ax.get_legend().remove()\n",
        "    else: sns.move_legend(ax, 'upper right') # for classification, label box\n",
        "\n",
        "    ax = plt.gca()\n",
        "    # Set the border or outline color and width\n",
        "    border_color = 'black'\n",
        "    border_width = 0.6  # Adjust this as needed\n",
        "\n",
        "    # Add a rectangular border around the plot\n",
        "    for i in ['top', 'right', 'bottom', 'left']: ax.spines[i].set_visible(True)\n",
        "\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_linewidth(border_width); spine.set_color(border_color)\n",
        "    # move the legend if has that:\n",
        "\n",
        "    if title == None: title = f'{dim_reduct}_demo'\n",
        "    plt.title(title)\n",
        "    if savepath != None:\n",
        "        make_path(savepath, False)\n",
        "        if savename == None: savename = f'{savepath}/{title}.png'\n",
        "        else: savename = savepath + savename\n",
        "        plt.savefig(savename, format='png', transparent=True)\n",
        "        print(f'figure saved at {savename}')\n",
        "    plt.show(); plt.close()\n",
        "\n",
        "\"\"\"TEST CODE\"\"\"\n",
        "# name = 'HIA_Hou'; IS_R = False\n",
        "# # name = 'Caco2_Wang'; IS_R = True\n",
        "# trains, valids, tests = collect_data_10_24([name], show_dist=False)\n",
        "# df_all   = pd.concat([trains, valids, tests], ignore_index=True, axis=0)\n",
        "# data_list = [trains, valids, tests, df_all]\n",
        "# desc_list = ['train set', 'valid set', 'test set', 'data set']\n",
        "# for (data, desc) in zip(data_list, desc_list):\n",
        "#     data = process(data)\n",
        "#     features, labels = data[header], data[name]\n",
        "#     assert features.shape[0] == len(labels)\n",
        "#     for dim_reduct in ['PCA', 't-SNE']:\n",
        "#         title = f'{dim_reduct} on {desc} of {name}'\n",
        "#         if dim_reduct == 'PCA':\n",
        "#             plot_dim_reduced(features, labels, IS_R, dim_reduct, title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpKDr3SLSM3K",
        "outputId": "23eb126c-1661-4ea3-e1a0-5c88418fbee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab type for RNN: smiles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TEST CODE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import UserList, defaultdict\n",
        "\n",
        "n_last = 1000\n",
        "n_batch = 512\n",
        "kl_start = 0\n",
        "kl_w_start = 0.0\n",
        "kl_w_end = 1.0 * 0.5 ###############################\n",
        "n_epoch = 500\n",
        "n_workers = 0\n",
        "\n",
        "clip_grad  = 50\n",
        "lr_start = 0.001\n",
        "lr_n_period = 50\n",
        "lr_n_mult = 1\n",
        "lr_end = 3 * 1e-4\n",
        "lr_n_restarts = 1 ###############################\n",
        "\n",
        "def get_collate_device(model): return model.device\n",
        "\n",
        "def get_optim_params(model):\n",
        "    return (p for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "class KLAnnealer:\n",
        "    def __init__(self,n_epoch):\n",
        "        self.i_start = kl_start\n",
        "        self.w_start = kl_w_start\n",
        "        self.w_max = kl_w_end\n",
        "        self.n_epoch = n_epoch\n",
        "\n",
        "        self.inc = (self.w_max - self.w_start) / (self.n_epoch - self.i_start)\n",
        "\n",
        "    def __call__(self, i):\n",
        "        k = (i - self.i_start) if i >= self.i_start else 0\n",
        "        return self.w_start + k * self.inc\n",
        "\n",
        "\n",
        "class CosineAnnealingLRWithRestart(_LRScheduler):\n",
        "    def __init__(self , optimizer):\n",
        "        self.n_period = lr_n_period\n",
        "        self.n_mult = lr_n_mult\n",
        "        self.lr_end = lr_end\n",
        "\n",
        "        self.current_epoch = 0\n",
        "        self.t_end = self.n_period\n",
        "\n",
        "        # Also calls first epoch\n",
        "        super().__init__(optimizer, -1)\n",
        "\n",
        "    def get_lr(self):\n",
        "        return [self.lr_end + (base_lr - self.lr_end) *\n",
        "                (1 + math.cos(math.pi * self.current_epoch / self.t_end)) / 2\n",
        "                for base_lr in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "        self.last_epoch = epoch\n",
        "        self.current_epoch += 1\n",
        "\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        if self.current_epoch == self.t_end:\n",
        "            self.current_epoch = 0\n",
        "            self.t_end = self.n_mult * self.t_end\n",
        "\n",
        "\n",
        "class CircularBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.max_size = size\n",
        "        self.data = np.zeros(self.max_size)\n",
        "        self.size = 0\n",
        "        self.pointer = -1\n",
        "\n",
        "    def add(self, element):\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.pointer = (self.pointer + 1) % self.max_size\n",
        "        self.data[self.pointer] = element\n",
        "        return element\n",
        "\n",
        "    def last(self):\n",
        "        assert self.pointer != -1, \"Can't get an element from an empty buffer!\"\n",
        "        return self.data[self.pointer]\n",
        "\n",
        "    def mean(self):\n",
        "        return self.data.mean()\n",
        "\n",
        "\n",
        "class Logger(UserList):\n",
        "    def __init__(self, data=None):\n",
        "        super().__init__()\n",
        "        self.sdata = defaultdict(list)\n",
        "        for step in (data or []):\n",
        "            self.append(step)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if isinstance(key, int):\n",
        "            return self.data[key]\n",
        "        elif isinstance(key, slice):\n",
        "            return Logger(self.data[key])\n",
        "        else:\n",
        "            ldata = self.sdata[key]\n",
        "            if isinstance(ldata[0], dict):\n",
        "                return Logger(ldata)\n",
        "            else:\n",
        "                return ldata\n",
        "\n",
        "    def append(self, step_dict):\n",
        "        super().append(step_dict)\n",
        "        for k, v in step_dict.items():\n",
        "            self.sdata[k].append(v)"
      ],
      "metadata": {
        "id": "-NRPc5bjLVBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def eval_VAE(model, loader, names, scale_dict=None):\n",
        "    if isinstance(names, str): names = [names]\n",
        "    IS_R = [names_dict[name] for name in names]\n",
        "    y_probs, y_label, mu_dict = {}, {}, {}\n",
        "    for idx, (input, labels) in enumerate(loader):\n",
        "        input, labels = input.to(model.device), labels.to(model.device)\n",
        "        mu, _ = model.encoder(input)\n",
        "        preds = model.classifier(mu)\n",
        "        # print(preds.shape)\n",
        "        mask = labels == MASK\n",
        "        del input\n",
        "        for j, (name, is_r) in enumerate(zip(names, IS_R)):\n",
        "            # mask_here = ~mask[:,j]\n",
        "            # MASK_here = mask_here.unsqueeze(1).repeat(1, mu.shape[1])\n",
        "\n",
        "            probs = preds[:,j][~mask[:,j]]\n",
        "            label = labels[:,j][~mask[:,j]]\n",
        "            mask_here = mask[:,j].reshape(mask[:, j].shape[0], 1).expand_as(mu)\n",
        "            mu_ = mu * (~mask_here)\n",
        "            del mask_here\n",
        "            # del mask_here; del MASK_here\n",
        "            if is_r == False: probs = F.sigmoid(probs)\n",
        "            probs = probs.cpu().detach().numpy().tolist()\n",
        "            label = label.cpu().detach().numpy().tolist()\n",
        "            mu_   = mu_.cpu().detach().numpy()\n",
        "\n",
        "            if scale_dict != None:\n",
        "                if name in scale_dict.keys():\n",
        "                    min_here = scale_dict[name][0]\n",
        "                    max_here = scale_dict[name][1]\n",
        "                    del_here = max_here - min_here\n",
        "                    label = [l*del_here + min_here for l in label]\n",
        "                    probs = [p*del_here + min_here for p in probs]\n",
        "            if idx == 0:\n",
        "                y_probs[name], y_label[name], mu_dict[name] = probs, label, mu_\n",
        "            else:\n",
        "                y_probs[name] += probs; y_label[name] += label\n",
        "                mu_dict[name] = np.append(mu_dict[name], mu_, axis=0)\n",
        "\n",
        "    return y_probs, y_label, mu_dict\n",
        "\n",
        "    # performance = eval_dict(y_probs, y_label, names, IS_R, draw_fig=False)\n",
        "\n",
        "    # for (name, is_r) in zip(names, IS_R):\n",
        "    #     infos = np.array(mu_dict[name])\n",
        "    #     # label = np.array(y_label[name])\n",
        "    #     # print(infos.shape)\n",
        "    #     label = pd.DataFrame()\n",
        "    #     label[name] = y_label[name]\n",
        "    #     # label = np.array(y_label[name])\n",
        "    #     # label = y_label[name]\n",
        "    #     title = f'{reduce_type} on {name}'\n",
        "    #     plot_dim_reduced(infos, label, is_r, reduce_type, title=title)\n"
      ],
      "metadata": {
        "id": "e_eFDzxWkHVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test VAE"
      ],
      "metadata": {
        "id": "Wot9skIgSes8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = ['CYP2C19_Veith',\n",
        " 'CYP2D6_Veith',\n",
        " 'CYP3A4_Veith',\n",
        " 'CYP1A2_Veith',\n",
        " 'CYP2C9_Veith']\n",
        "\n",
        "trn, val, tst = collect_data(name)\n",
        "scale_here = False\n",
        "trn, val, tst, dict_scale = scale(trn,val,tst, scale_here)\n",
        "\n",
        "config = get_config('VAE', name)\n",
        "config['scale_dict'] = dict_scale\n",
        "trn_l, val_l, tst_l, vocab = get_multi_loader(trn, val, tst, config)"
      ],
      "metadata": {
        "id": "7_9qatlR_KRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config['verbose_freq'] = 10\n",
        "models = PRED(**config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnQfbtKeALEZ",
        "outputId": "3b8ff030-370d-4343-9ed5-87bd59306ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model type:  VAE | Model parameters:  7889067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models.train(trn_l, val_l, tst_l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWYUfljp_rK2",
        "outputId": "b78c2547-f181-426a-cafc-667009379498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training VAE...\n"
          ]
        }
      ]
    }
  ]
}