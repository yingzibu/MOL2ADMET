{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9OQtI6u9IKMvWtGRhdzjF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yingzibu/MOL2ADMET/blob/main/examples/experiments/RNN/ADMET_11_01_selfies_vocab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NPc6PNRI--I",
        "outputId": "d01e8fd1-bed2-4ae7-f4c2-4b71543d61b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install PyTDC --quiet\n",
        "! pip install dgllife --quiet\n",
        "! pip install dgl -f https://data.dgl.ai/wheels/cu118/repo.html --quiet\n",
        "! pip install dglgo -f https://data.dgl.ai/wheels-test/repo.html --quiet\n",
        "! pip install mycolorpy --quiet\n",
        "! pip install rdkit --quiet\n",
        "! pip install DeepPurpose --quiet\n",
        "! pip install git+https://github.com/bp-kelley/descriptastorus --quiet\n",
        "! pip install pandas-flavor --quiet\n",
        "! pip install selfies --quiet"
      ],
      "metadata": {
        "id": "bdy7Ef1HJXMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/ADMET"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxhbDR3KJHoL",
        "outputId": "f1bf782d-7945-419b-9fb6-c83da9acfbfc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ADMET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN get vocab, test selfies"
      ],
      "metadata": {
        "id": "XNKMqSN64HP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tdc.generation import MolGen\n",
        "data = MolGen(name = 'ChEMBL')\n",
        "split = data.get_split()\n",
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "for i in ['MOSES', 'ZINC', 'ChEMBL']:\n",
        "    data = MolGen(name = i)\n",
        "    split = data.get_split()\n",
        "    trains, valids, tests = split['train'], split['valid'], split['test']\n",
        "    df = pd.concat([trains, valids, tests, df], ignore_index=True, axis=0)\n",
        "    print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDW0obPklc07",
        "outputId": "05a2fdf3-01bd-492b-b2e4-6fe063bdfeae"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found local copy...\n",
            "Loading...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import selfies as sf\n",
        "from tqdm import tqdm\n",
        "def convert_smi_to_sf(smi_list):\n",
        "    sf_list = []\n",
        "    # valid_smi_list = []\n",
        "    for smi in tqdm(smi_list, total=len(smi_list),\n",
        "                    desc='converting smiles -> selfies'):\n",
        "        try:\n",
        "            drug_sf = sf.encoder(smi)\n",
        "            drug_sm = sf.decoder(drug_sf)\n",
        "            sf_list.append(drug_sf)\n",
        "            # valid_smi_list.append()\n",
        "        except:\n",
        "            print('cannot handle ', smi)\n",
        "    return sf_list\n",
        "\n",
        "def get_vocab(train: pd.DataFrame, vocab_type=VOCAB_TYPE):\n",
        "\n",
        "    df = train.copy()\n",
        "    smiles = []\n",
        "    for col_smi in ['smiles', 'Drug', 'SMILES', 'Smiles', 'smile']:\n",
        "        if col_smi in df.columns: smiles = list(df[col_smi]); break\n",
        "    if len(smiles) == 0:\n",
        "        print('no smile info!'); return\n",
        "\n",
        "    assert isinstance(smiles, list) == True\n",
        "    assert len(df) == len(smiles)\n",
        "\n",
        "    selfies = []\n",
        "    if vocab_type == 'selfies':\n",
        "        for col_sf in ['selfies', 'SELFIES']:\n",
        "            if col_sf in df.columns: drug_sfs = list(df[col_sf]); break\n",
        "        if len(selfies) == 0: # need to calculate\n",
        "            drug_sfs = convert_smi_to_sf(smiles)\n",
        "        try:\n",
        "            alphabet = sf.get_alphabet_from_selfies(drug_sfs)\n",
        "        except:\n",
        "            print('error get alphabet for selfies! will return sfs for checking')\n",
        "            return drug_sfs\n",
        "        alphabet = sorted(alphabet)\n",
        "\n",
        "    else: # char or smiles\n",
        "        chars = set()\n",
        "        for string in smiles:\n",
        "            try:\n",
        "                if vocab_type == 'char': chars.update(string) # create an alphabet set\n",
        "                elif vocab_type == 'smiles':\n",
        "                    chars.update(smiles_tokenizer(string))\n",
        "            except: pass\n",
        "        alphabet = sorted(list(chars))\n",
        "    all_sys =  ['<pad>', '<bos>', '<eos>', '<unk>'] + alphabet\n",
        "    print('alphabet len: ', len(alphabet), ' add helper token: ', len(all_sys))\n",
        "    return all_sys\n",
        "get_vocab(trains, 'selfies')"
      ],
      "metadata": {
        "id": "258P8PnrmLDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save char vocab at vocab/char.yml"
      ],
      "metadata": {
        "id": "QLiEJFeDt-dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = get_vocab(df, vocab_type='char')\n",
        "data = dict(vocab=vocab, vocab_type='char')\n",
        "with open('vocab/char.yml', 'w') as f:\n",
        "    yaml.dump(data, f, default_flow_style=False)"
      ],
      "metadata": {
        "id": "y4rZeTt8nlf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check saved smiles vocab at vocab/smiles.yml"
      ],
      "metadata": {
        "id": "3aFvfZx-uEnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('vocab/smiles.yml', 'r') as f:\n",
        "    data = yaml.safe_load(f)"
      ],
      "metadata": {
        "id": "4h1vDIqJoj7f"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get selfies vovab"
      ],
      "metadata": {
        "id": "Ckma_aIvuLEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first check ADMET dataset, collect all smiles and convert to selfies\n",
        "smi_list = []\n",
        "for name in names_all:\n",
        "    print(name)\n",
        "    trains, valids, tests = collect_data(name)\n",
        "    smi_list += list(trains['Drug'])\n",
        "    smi_list += list(valids['Drug'])\n",
        "    smi_list += list(tests['Drug'])\n",
        "    smi_list = [*set(smi_list)]\n",
        "    print(f'after {name}, smi_list len: ', len(smi_list))\n",
        "\n",
        "pred_df = pd.DataFrame()\n",
        "pred_df['smiles'] = smi_list\n",
        "sf_list = convert_smi_to_sf(smi_list)\n",
        "alphabet = sf.get_alphabet_from_selfies(sf_list)\n",
        "# sf_list = convert_smi_to_sf(smi_list[:10000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUf_LngtqAb2",
        "outputId": "17403503-5a90-445a-83df-e6c14910ea01"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "converting smiles -> selfies: 100%|██████████| 10000/10000 [00:19<00:00, 522.37it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(alphabet) # alphabet too large, do a count and collect top 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PNOqWuGqDjV",
        "outputId": "152f0d4e-6a4b-4854-b3f4-5f3e6aa134e9"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "275"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok_dict = {}\n",
        "for drug_sf in tqdm(sf_list, total=len(sf_list), desc='counting token frequency'):\n",
        "    toks = list(sf.split_selfies(drug_sf))\n",
        "    for tok in toks:\n",
        "        if tok in tok_dict.keys():\n",
        "            tok_dict[tok] += 1\n",
        "        else: tok_dict[tok] = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olRHAKuluw9e",
        "outputId": "fb3bbd1a-2965-45c7-87ec-3650528cfeb3"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "counting token frequency: 100%|██████████| 58441/58441 [00:02<00:00, 21091.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_tok_dict = sorted(tok_dict.items(), key=lambda x:x[1], reverse=True)\n",
        "sorted_tok_dict = dict(sorted_tok_dict)\n",
        "alphabet_list = []\n",
        "for i, j in sorted_tok_dict.items():\n",
        "    if j > 10:\n",
        "        alphabet_list.append(i)\n",
        "print(len(alphabet_list)) # 100\n",
        "alphabet = sorted(alphabet_list)\n",
        "all_sys =  ['<pad>', '<bos>', '<eos>', '<unk>'] + alphabet\n",
        "print('after add helper tokens: alphabet len : ', len(all_sys) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXdcqBk1vVlk",
        "outputId": "1a74c9e5-423c-48e0-8295-ae9102d4937e"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_vocab(pred_df, 'selfies')"
      ],
      "metadata": {
        "id": "pYPndyE1rIzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dict(vocab=all_sys, vocab_type='selfies')\n",
        "with open('vocab/selfies.yml', 'w') as f:\n",
        "    yaml.dump(data, f, default_flow_style=False)"
      ],
      "metadata": {
        "id": "Gm25ote7rs5D"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def string2ids(string, c2i, add_bos=False, add_eos=False, vocab_type=VOCAB_TYPE):\n",
        "    if vocab_type == 'char': ids = [char2id(c, c2i) for c in string]\n",
        "    elif vocab_type == 'smiles':\n",
        "        tokens = smiles_tokenizer(string)\n",
        "        ids = [char2id(t, c2i) for t in tokens]\n",
        "    elif vocab_type == 'selfies': # selfies\n",
        "        try:\n",
        "            drug_sf = sf.encoder(string)\n",
        "            drug_smi = sf.decoder(drug_sf)\n",
        "            tokens = list(sf.split_selfies(drug_sf))\n",
        "        except: tokens = []\n",
        "        ids = [char2id(t, c2i) for t in tokens]\n",
        "    else: print('Error, not valid vocab_type!'); return\n",
        "    if add_bos: ids = [c2i['<bos>']] + ids\n",
        "    if add_eos: ids = ids + [c2i['<eos>']]\n",
        "    return ids\n",
        "\n",
        "def ids2string(ids, c2i, i2c, rem_bos=True, rem_eos=True):\n",
        "    # print(ids)\n",
        "    if isinstance(ids[0], list): ids = ids[0]\n",
        "    if len(ids) == 0: return ''\n",
        "    if rem_bos and ids[0] == c2i['<bos>']: ids = ids[1:]\n",
        "    # delete <eos>\n",
        "    if rem_eos:\n",
        "        for i, id in enumerate(ids):\n",
        "            # print(i, id)\n",
        "            if id == c2i['<eos>']: ids = ids[:i]; break\n",
        "    string = ''.join([id2char(id, i2c, c2i) for id in ids])\n",
        "    return string\n",
        "\n",
        "def string2tensor(string, c2i, device='cpu'):\n",
        "    # c2i, i2c = get_c2i_i2c(vocab)\n",
        "    ids = string2ids(string, c2i, add_bos=True, add_eos=True)\n",
        "    tensor = torch.tensor(ids, dtype=torch.long, device=device)\n",
        "    return tensor\n",
        "\n",
        "c2i, i2c = get_c2i_i2c(all_sys)\n",
        "ids = string2ids(smi, c2i, vocab_type='selfies', add_bos=True, add_eos=True)\n",
        "ids"
      ],
      "metadata": {
        "id": "AGZ7FQfrruX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def string2tensor(string, c2i, vocab_type=VOCAB_TYPE, device='cpu'):\n",
        "    # c2i, i2c = get_c2i_i2c(vocab)\n",
        "    ids = string2ids(string, c2i, add_bos=True, add_eos=True,\n",
        "                     vocab_type=vocab_type)\n",
        "    tensor = torch.tensor(ids, dtype=torch.long, device=device)\n",
        "    return tensor\n",
        ""
      ],
      "metadata": {
        "id": "tk0BDA2Kr0ZE"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sf.decoder(ids2string(ids, c2i, i2c)), smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA8GUf8-0_Vi",
        "outputId": "70e484b1-59c1-4ce7-ac8c-5c14096a8de8"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('O=[N+1]([O-1])C1=C2C(=C3C=CC4=CC=CC5=CC=C1C3=C45)CCCC2',\n",
              " 'O=[N+]([O-])c1c2c(c3ccc4cccc5ccc1c3c45)CCCC2')"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string2tensor(smi, c2i, vocab_type='selfies')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Arx2uP0l2Xe1",
        "outputId": "79222157-b8a1-4cf3-c94c-569f842f7d9d"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1, 82, 21, 39, 47, 79, 47, 20, 47, 19, 87, 18, 20, 47, 20, 47, 20, 47,\n",
              "        20, 47, 20, 47, 20, 87, 20, 47, 87, 76, 20, 87,  6, 87, 18, 47, 47, 47,\n",
              "        47, 88, 87, 87,  2])"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# invalid still convertable\n",
        "string2tensor('78232fa64f', c2i, vocab_type='selfies')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_riOF3uk3tFD",
        "outputId": "6c5f95fd-3abb-43b7-98fc-9c289d75ae9b"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smi = 'O=[N+]([O-])c1c2c(c3ccc4cccc5ccc1c3c45)CCCC2'\n",
        "drug_sf = sf.encoder(smi)\n",
        "print(drug_sf)\n",
        "string2ids(smi, c2i, 'selfies')"
      ],
      "metadata": {
        "id": "M1EdE8mZspNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## yaml test"
      ],
      "metadata": {
        "id": "3oGnffnu4Zkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "yml_name = '/content/drive/MyDrive/ADMET/cls/BBB_Martins/RNN_ST_0.yml'\n",
        "print(yml_name)\n",
        "p1 = yml_report(yml_name, ver=False)\n",
        "eval_perf_list(p1, 'BBB_Martins', d)\n",
        "\n",
        "yml_name = '/content/drive/MyDrive/ADMET/cls/BBB_Martins/RNN_ST_1.yml'\n",
        "print(yml_name)\n",
        "p2 = yml_report(yml_name, ver=False)\n",
        "eval_perf_list(p2, 'BBB_Martins', d)\n",
        "\n",
        "eval_perf_list([p1, p2], 'BBB_Martins', {})\n",
        "eval_perf_list([p1, p2], 'BBB_Martins', d)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkY_JckBN-iW",
        "outputId": "f6a8c723-db76-4507-f516-fd8c131a319c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ADMET/cls/BBB_Martins/RNN_ST_0.yml\n",
            "******************** BBB_Martins ******************** \n",
            "\t|       acc      |       f1      |       auc      \n",
            "single: &0.830            &0.892            &0.834            \n",
            "/content/drive/MyDrive/ADMET/cls/BBB_Martins/RNN_ST_1.yml\n",
            "******************** BBB_Martins ******************** \n",
            "\t|       acc      |       f1      |       auc      \n",
            "single: &0.835            &0.896            &0.854            \n",
            "repeated num # 2 idx 1 has the lowest loss from [1.5305130779743195, 1.5076239258050919]\n",
            "******************** BBB_Martins ******************** \n",
            "\t|       acc      |       w_acc      |       prec      |       recall      |       sp      |       f1      |       auc      |       mcc      |       ap      \n",
            "\t&0.833$\\pm$0.002  &0.720$\\pm$0.000  &0.846$\\pm$0.001  &0.949$\\pm$0.005  &0.490$\\pm$0.005  &0.894$\\pm$0.002  &0.844$\\pm$0.010  &0.518$\\pm$0.006  &0.930$\\pm$0.008  \n",
            " idx 1: &0.835            &0.720            &0.845            &0.954            &0.485            &0.896            &0.854            &0.524            &0.938            \n",
            "\n",
            "repeated num # 2 idx 1 has the lowest loss from [1.5305130779743195, 1.5076239258050919]\n",
            "******************** BBB_Martins ******************** \n",
            "\t|       acc      |       f1      |       auc      \n",
            "\t&0.833$\\pm$0.002  &0.894$\\pm$0.002  &0.844$\\pm$0.010  \n",
            " idx 1: &0.835            &0.896            &0.854            \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scripts.yaml_utils import *\n",
        "#                0      1        2       3        4     5      6      7      8\n",
        "cls_metrics = ['acc', 'w_acc', 'prec', 'recall', 'sp', 'f1', 'auc', 'mcc', 'ap']\n",
        "reg_metrics = ['mae', 'mse',   'rmse', 'r2']\n",
        "d = {'reg': [0,   2,    3], 'cls': [0,   5,  6]}\n",
        "#            mae, rmse, r2          acc, f1, auc"
      ],
      "metadata": {
        "id": "zFOf_RNOJMyQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yml_name = 'Lipophilicity_AstraZeneca_scale/AttentiveFP_ST_1.yml'\n",
        "p = yml_report(yml_name, ver=False)\n",
        "print('Evaluate all metrics, just set metrics_dict as {}')\n",
        "\n",
        "print(yml_name)\n",
        "eval_perf_list(p, 'Lipophilicity_AstraZeneca', metrics_dict={})\n",
        "\n",
        "print(f'\\nEvaluate selected metrics {d}')\n",
        "eval_perf_list(p, 'Lipophilicity_AstraZeneca', metrics_dict=d)\n",
        "\n",
        "print('\\nevaluate multiple results')\n",
        "eval_perf_list([p, p], 'Lipophilicity_AstraZeneca')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJQRMda8LoNO",
        "outputId": "1b1cf823-bd85-48ee-e6c2-cd9284896b91"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate all metrics, just set metrics_dict as {}\n",
            "Lipophilicity_AstraZeneca_scale/AttentiveFP_ST_1.yml\n",
            "******************** Lipophilicity_AstraZeneca ******************** \n",
            "\t|       mae      |       mse      |       rmse      |       r2      \n",
            "single: &0.353            &0.239            &0.489            &0.834            \n",
            "\n",
            "Evaluate selected metrics {'reg': [0, 2, 3], 'cls': [0, 5, 6]}\n",
            "******************** Lipophilicity_AstraZeneca ******************** \n",
            "\t|       mae      |       rmse      |       r2      \n",
            "single: &0.353            &0.489            &0.834            \n",
            "\n",
            "evaluate multiple results\n",
            "repeated num # 2 idx 0 has the lowest loss from [0.006638023186297644, 0.006638023186297644]\n",
            "******************** Lipophilicity_AstraZeneca ******************** \n",
            "\t|       mae      |       rmse      |       r2      \n",
            "\t&0.353$\\pm$0.000  &0.489$\\pm$0.000  &0.834$\\pm$0.000  \n",
            " idx 0: &0.353            &0.489            &0.834            \n",
            "\n"
          ]
        }
      ]
    }
  ]
}