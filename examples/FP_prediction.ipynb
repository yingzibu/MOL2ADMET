{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yingzibu/MOL2AE/blob/main/examples/FP_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl0L0e_PZQ6J"
      },
      "source": [
        "#### NEED TO DO\n",
        "\n",
        "1. train:val:test split. Not done here. Used all drugs to do training (since the data is few)\n",
        "2. label should be one hot encoding and use BCELoss, here just use the original [1,2,3,4,5] and used MSELoss, need to be changed\n",
        "3. Did not incorporate protein expression info. Could try simple multiplication on IC$_{50}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMROh0pLDKVc",
        "outputId": "7391c3ed-7e43-4b04-e081-f1665cfaabcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/tox_pred\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/tox_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEyanyriDPM9",
        "outputId": "aa86be0b-bf14-47f1-d74f-1bf7237975b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drug_reaction_freq.tsv  drug_smiles_ic50.tsv  hpa_gene_seqs.json\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO4zpBLdJWCr"
      },
      "outputs": [],
      "source": [
        "! rm -R 'JAK_ML'\n",
        "! git clone https://github.com/yingzibu/JAK_ML.git --quiet\n",
        "!rm -R 'a_inhibitor_design'\n",
        "! git clone https://github.com/yingzibu/a_inhibitor_design.git --quiet\n",
        "! pip install pubchempy --quiet\n",
        "! pip install rdkit --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFsIubqNEiF6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "drug_freq = pd.read_table('drug_reaction_freq.tsv')\n",
        "drug_freq = drug_freq.iloc[:, :-1]\n",
        "drug_smiles = pd.read_table('drug_smiles_ic50.tsv')\n",
        "drug_smiles = drug_smiles.iloc[:, :-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgeVLqJdInDV"
      },
      "outputs": [],
      "source": [
        "df_all = drug_smiles.merge(drug_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmSdl6N6EoLz"
      },
      "outputs": [],
      "source": [
        "# JUST SELETED A SUBSET OF IC50 AND FREQUENCY TO TEST CODE\n",
        "small_drug_smiles = drug_smiles.iloc[:, :10]\n",
        "small_drug_freq = drug_freq.iloc[:, :10]\n",
        "small_df = small_drug_smiles.merge(small_drug_freq)\n",
        "\n",
        "small_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4DCq9_3FBj0"
      },
      "outputs": [],
      "source": [
        "from JAK_ML.ML_code.function import *\n",
        "def process(df):\n",
        "    MACCS_list = smile_list_to_MACCS(df['SMILES'].tolist())\n",
        "    header = ['bit' + str(i) for i in range(167)]\n",
        "    new_df = pd.DataFrame(MACCS_list, columns=header)\n",
        "    new_df['SMILES'] = df['SMILES']\n",
        "    new_df = new_df.merge(df)\n",
        "    return new_df\n",
        "data_df = process(df_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25Itj1EmFgjx"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super(Classifier, self).__init__()\n",
        "        [in_dim, h_dims, out_dim] = dims\n",
        "        self.dims = dims\n",
        "        neurons = [in_dim, *h_dims]\n",
        "        linear_layers = [nn.Linear(neurons[i-1], neurons[i]) \\\n",
        "                         for i in range(1, len(neurons))]\n",
        "        self.hidden = nn.ModuleList(linear_layers)\n",
        "        # self.emb = nn.GRU(h_dims[-1], h_dims[-1])\n",
        "        self.final = nn.Linear(h_dims[-1], out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden:\n",
        "            x = F.relu(layer(x))\n",
        "        x = self.final(x)\n",
        "        return x\n",
        "\n",
        "def onehot(k):\n",
        "    def encode(label):\n",
        "        y = torch.zeros(len(label), k+1)\n",
        "        for i, j  in enumerate(label):\n",
        "            if j.item() <= k: y[i][int(j.item())] = 1\n",
        "        return y[:, 1:]\n",
        "    return encode\n",
        "# onehot(5)(ae)\n",
        "\n",
        "class tox_dataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.len = len(df)\n",
        "        self.df = df\n",
        "        self.ic_start_ind = df.columns.get_loc(\"TSPAN6\")\n",
        "        self.ae_start_ind = df.columns.get_loc('asthenia')\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        OUTPUT\n",
        "        :param fp: fingerprint, should be 167 dim\n",
        "        :param ic: ic50\n",
        "        :param ae: adverse events\n",
        "        \"\"\"\n",
        "        header = ['bit' + str(i) for i in range(167)]\n",
        "        fp = self.df[header]\n",
        "        fp = torch.tensor([float(b) for b in fp.iloc[idx]], dtype=torch.float32)\n",
        "        ic = self.df.iloc[:, self.ic_start_ind:self.ae_start_ind]\n",
        "        ic = torch.tensor(ic.values.astype(np.float32))[idx]\n",
        "        ae = self.df.iloc[:, self.ae_start_ind:]\n",
        "        ae = torch.tensor(ae.values.astype(np.float32))[idx]\n",
        "        # ae = onehot(5)(ae)\n",
        "        return fp, ic, ae\n",
        "    def __len__(self): return self.len\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xin9r0WkFtDL"
      },
      "outputs": [],
      "source": [
        "# dataset test, select first row\n",
        "fp, ic, ae = tox_dataset(data_df)[0]\n",
        "fp, ic, ae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmH90Xw3MYOF",
        "outputId": "90892fa3-357c-4f7c-819c-25e5bb6db2e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[15171, [512, 256, 128], 994]"
            ]
          },
          "execution_count": 203,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# hidden dims of neural network are changeable,\n",
        "# as long as its all integers\n",
        "h_dims = [256*2, 256, 128]\n",
        "##############################################\n",
        "\n",
        "\n",
        "# BELOW IS NOT CHANGEABLE\n",
        "in_dim = fp.shape[0] + ic.shape[0]\n",
        "out_dim = ae.shape[0]\n",
        "\n",
        "dims = [in_dim, h_dims, out_dim]\n",
        "dims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvpu3UFXMiOO"
      },
      "outputs": [],
      "source": [
        "# batch_size is changeable\n",
        "params = {'batch_size':16, 'shuffle':True,\n",
        "          'drop_last':False, 'num_workers': 0}\n",
        "\n",
        "train_loader = DataLoader(tox_dataset(data_df), **params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEiQ8CzNP00c"
      },
      "outputs": [],
      "source": [
        "model = Classifier(dims)\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda: model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbT_BsN3M7CZ"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "loss_function = nn.MSELoss() # better use cross entropy, need onehot encoding\n",
        "\n",
        "# parameter changeable:\n",
        "lr = 1e-3 # learning rate\n",
        "wd = 1e-3 # weight decay\n",
        "#######################\n",
        "\n",
        "optimizer = optim.AdamW(params=model.parameters(), lr=lr, weight_decay=wd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnHjLtc9dPLM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def get_min(d:dict):\n",
        "    min_key = next(iter(d))\n",
        "    for key in d:\n",
        "        if d[key] < d[min_key]: min_key = key\n",
        "    return min_key, d[min_key]\n",
        "\n",
        "def plot_loss(a:dict):\n",
        "    plt.plot(a.keys(), a.values())\n",
        "    argmin, min = get_min(a)\n",
        "    plt.plot(argmin, min, '*', label=f'min epoch: {argmin}')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.title('Loss during training')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NYXDOUdpP4ki"
      },
      "outputs": [],
      "source": [
        "epochs = 1000\n",
        "min_loss = 100000\n",
        "\n",
        "loss_dict = {}\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for i, (fp, ic, ae) in tqdm(enumerate(train_loader),\n",
        "                                total=len(train_loader),\n",
        "                            desc=f'Epoch {epoch}'):\n",
        "        optimizer.zero_grad()\n",
        "        if cuda: fp, ic, ae = fp.cuda(), ic.cuda(), ae.cuda()\n",
        "        mask = ae == -100\n",
        "        output = model(torch.cat((fp, ic), 1))\n",
        "        loss = loss_function(output[~mask], ae[~mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    total_loss /= len(train_loader)\n",
        "    print(f'[Train] MSE loss: {total_loss:.3f}')\n",
        "    loss_dict[epoch] = total_loss\n",
        "    if total_loss < min_loss:\n",
        "        min_loss = total_loss\n",
        "        torch.save(model.state_dict(), 'ckpt.pt')\n",
        "\n",
        "    # Employ early stop\n",
        "    # training too long yet no loss decrease:  STOP\n",
        "    argmin, min = get_min(loss_dict)\n",
        "    if epoch - argmin > 10:\n",
        "        print('no decrease after 10 epoch, early stop')\n",
        "        break\n",
        "\n",
        "    # training yet see loss increase too much: STOP\n",
        "    if total_loss / min > 1.3:\n",
        "        print('loss increase 30% compared with min loss, early stop')\n",
        "        break\n",
        "\n",
        "    if epoch % 10 == 0 and epoch != 0:\n",
        "        plot_loss(loss_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGcXNG7oeRQQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1iBvDGytAxY9Ylehf6KVkOhl9Gndg-Cd_",
      "authorship_tag": "ABX9TyPyUy1m0gFhUaYaLKFpDlRY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}